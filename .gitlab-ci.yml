# Configuration borrowed from https://gitlab.cern.ch/db/swan-spark-notebooks
before_script:
# Converter from .py to ipynb
  - pip install jupytext
  - mkdir -p ~/.ssh
  - echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config
  - eval $(ssh-agent -s)
  - echo "$SSH_DEPLOY_KEY" | tr -d '\r' | ssh-add - > /dev/null
  - mkdir -p ~/.ssh && touch ~/.ssh/known_hosts
  - echo "$SSH_KNOWN_HOSTS" >> ~/.ssh/known_hosts
  - git submodule sync --recursive
  - git submodule update --init --recursive  --remote

stages:
  - image-build
  - submit-test-notebook
  - submit-adl-notebook
  - submit-dimuon-notebook
  - submit-pyspark-notebook

variables:
  HADOOP_CONFEXT_QA_BRANCH: "testing"
  KRB5CCNAME: "/tmp/krb5cc"

### COMMON ###

# Deploy docker image to gitlab registry
build-docker-image:
  stage: image-build
  only:
    - /^docker-.*$/
  tags:
    - docker-image-build
  script:
    - echo "Build docker image..."
  variables:
    DOCKER_FILE: Dockerfile
    TO: ${CI_REGISTRY_IMAGE}:latest

# Template for the job
.job_template: &job_template
  tags:
    - cvmfs
  image:
    name: ${CI_REGISTRY_IMAGE}:latest
    entrypoint: ["/bin/sh", "-c"]

convert-notebooks:
  stage: .pre
  script:
    - print "Start!"

submit-test-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-test-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./swan-spark-notebooks/k8s-example/spark-pi.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl1-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
     --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
     --ipynb ./benchmarks/test_coffea_laurelin_adl_example1.ipynb \
     --appid prod-docker-image-notebook-$RANDOM \
     --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl2-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
     --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
     --ipynb ./benchmarks/test_coffea_laurelin_adl_example2.ipynb \
     --appid prod-docker-image-notebook-$RANDOM \
     --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl3-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./benchmarks/test_coffea_laurelin_adl_example3.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl4-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./benchmarks/test_coffea_laurelin_adl_example4.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl5-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./benchmarks/test_coffea_laurelin_adl_example5.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl6-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./benchmarks/test_coffea_laurelin_adl_example6.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl7-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./benchmarks/test_coffea_laurelin_adl_example7.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-adl8-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-adl-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./sbenchmarks/test_coffea_laurelin_adl_example8.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-dimuon-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-dimuon-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./benchmarks/test_pyspark_laurelin_trees.ipynb \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

submit-pyspark-trees-notebook:
  <<: *job_template
  image:
    name: gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2
    entrypoint: ["/bin/sh", "-c"]
  stage: submit-pyspark-notebook
  script:
    # set KUBECONFIG
    - cp ./swan-spark-notebooks/k8s-example/kubeconfig /tmp/kubeconfig
    - sed -i -e "s/K8S_SSL_CERTIFICATE/$K8S_SSL_CERTIFICATE/g" /tmp/kubeconfig; sed -i -e "s,K8S_MASTER,$K8S_MASTER,g" /tmp/kubeconfig; sed -i -e "s/K8S_TOKEN/$K8S_TOKEN/g" /tmp/kubeconfig
    - export KUBECONFIG=/tmp/kubeconfig
    # submit notebook
    - python ./swan-spark-notebooks/k8s-example/submit_k8s_dockerfile_ipynb.py \
      --spark-image gitlab-registry.cern.ch/db/spark-service/docker-registry/spark:v2.4.4-hadoop2 \
      --ipynb ./benchmarks/test_pyspark_laurelin_trees.py \
      --appid prod-docker-image-notebook-$RANDOM \
      --hadoop-confext /cvmfs/sft.cern.ch/lcg/etc/hadoop-confext

collect-resuls:
  stage: .post
  script:
    - print "Done!"