{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: coffea in /eos/home-o/oshadura/.local/lib/python3.6/site-packages (0.6.33)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27.0 in /eos/home-o/oshadura/.local/lib/python3.6/site-packages (from coffea) (4.38.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/swan (from coffea) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: ipywidgets in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (7.4.2)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: awkward>=0.12.20 in /eos/home-o/oshadura/.local/lib/python3.6/site-packages (from coffea) (0.12.20)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (19.0)\n",
      "Requirement already satisfied, skipping upgrade: mplhep!=0.0.37,>=0.0.16 in /eos/home-o/oshadura/.local/lib/python3.6/site-packages (from coffea) (0.0.16)\n",
      "Requirement already satisfied, skipping upgrade: numba>=0.43.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (0.44.0)\n",
      "Requirement already satisfied, skipping upgrade: lz4 in /eos/home-o/oshadura/.local/lib/python3.6/site-packages (from coffea) (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: uproot-methods>=0.7.3 in /eos/home-o/oshadura/.local/lib/python3.6/site-packages (from coffea) (0.7.3)\n",
      "Requirement already satisfied, skipping upgrade: uproot>=3.11.0 in /eos/home-o/oshadura/.local/lib/python3.6/site-packages (from coffea) (3.11.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=3 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from coffea) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/swan (from pandas->coffea) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from pandas->coffea) (2019.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from packaging->coffea) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: requests~=2.21 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from mplhep!=0.0.37,>=0.0.16->coffea) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: llvmlite>=0.29.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from numba>=0.43.1->coffea) (0.29.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from uproot>=3.11.0->coffea) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from matplotlib>=3->coffea) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from matplotlib>=3->coffea) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from requests~=2.21->mplhep!=0.0.37,>=0.0.16->coffea) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from requests~=2.21->mplhep!=0.0.37,>=0.0.16->coffea) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from requests~=2.21->mplhep!=0.0.37,>=0.0.16->coffea) (1.25.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /cvmfs/sft.cern.ch/lcg/views/LCG_96python3/x86_64-centos7-gcc8-opt/lib/python3.6/site-packages (from requests~=2.21->mplhep!=0.0.37,>=0.0.16->coffea) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/swan (from kiwisolver>=1.0.1->matplotlib>=3->coffea) (41.6.0)\n",
      "--2020-02-19 13:08:36--  https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.0.0/laurelin-1.0.0.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1708408 (1.6M) [application/java-archive]\n",
      "Server file no newer than local file ‘laurelin-1.0.0.jar’ -- not retrieving.\n",
      "\n",
      "--2020-02-19 13:08:38--  https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 266283 (260K) [application/java-archive]\n",
      "Server file no newer than local file ‘log4j-api-2.11.2.jar’ -- not retrieving.\n",
      "\n",
      "--2020-02-19 13:08:40--  https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1629585 (1.6M) [application/java-archive]\n",
      "Server file no newer than local file ‘log4j-core-2.11.2.jar’ -- not retrieving.\n",
      "\n",
      "--2020-02-19 13:08:41--  https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 620174 (606K) [application/java-archive]\n",
      "Server file no newer than local file ‘lz4-java-1.5.1.jar’ -- not retrieving.\n",
      "\n",
      "--2020-02-19 13:08:43--  https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 94816 (93K) [application/java-archive]\n",
      "Server file no newer than local file ‘xz-1.2.jar’ -- not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.0.0/laurelin-1.0.0.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pyspark.sql\n",
    "from pyarrow.compat import guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be changed\n",
    "partitionsize = 20000\n",
    "# parameters to be changed\n",
    "thread_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'Jets': { 'files': ['root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root'],\n",
    "             'treename': 'Events'\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fileset = {'MET': ['root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# parameters to be changed\n",
    "available_laurelin_version = [(\"edu.vanderbilt.accre:laurelin:1.0.0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# This program plots an event-level variable (in this case, MET, but switching it is as easy as a dict-key change). It also demonstrates an easy use of the book-keeping cutflow tool, to keep track of the number of events processed.\n",
    "# The processor class bundles our data analysis together while giving us some helpful tools.  It also leaves looping and chunks to the framework instead of us.\n",
    "class METProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        # Bins and categories for the histogram are defined here. For format, see https://coffeateam.github.io/coffea/stubs/coffea.hist.hist_tools.Hist.html && https://coffeateam.github.io/coffea/stubs/coffea.hist.hist_tools.Bin.html\n",
    "        self._columns = ['MET_pt']\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        MET_axis = hist.Bin(\"MET\", \"MET [GeV]\", 50, 0, 100)\n",
    "        # The accumulator keeps our data chunks together for histogramming. It also gives us cutflow, which can be used to keep track of data.\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'MET': hist.Hist(\"Counts\", dataset_axis, MET_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int)\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        # This is where we do our actual analysis. The df has dict keys equivalent to the TTree's.\n",
    "        dataset = df['dataset']\n",
    "        MET = df['MET_pt'] \n",
    "        # We can define a new key for cutflow (in this case 'all events'). Then we can put values into it. We need += because it's per-chunk (demonstrated below)\n",
    "        output['cutflow']['all events'] += MET.size\n",
    "        output['cutflow']['number of chunks'] += 1 \n",
    "        # This fills our histogram once our data is collected. Always use .flatten() to make sure the array is reduced. The output key will be as defined in __init__ for self._accumulator; the hist key ('MET=') will be defined in the bin.\n",
    "        output['MET'].fill(dataset=dataset, MET=MET.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def coffea_laurelin_adl_example1(laurelin_version, fileset):\n",
    "    spark_config = pyspark.sql.SparkSession.builder \\\n",
    "        .appName('spark-executor-test-%s' % guid()) \\\n",
    "        .master('local[*]') \\\n",
    "        .config('spark.driver.memory', '4g') \\\n",
    "        .config('spark.executor.memory', '6g') \\\n",
    "        .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "        .config('spark.sql.execution.arrow.maxRecordsPerBatch', 20000)\\\n",
    "        .config('spark.driver.extraClassPath', './laurelin-1.0.0.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar')\\\n",
    "        .config('spark.kubernetes.container.image.pullPolicy', 'true')\\\n",
    "        .config('spark.kubernetes.container.image', 'gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin')\\\n",
    "        .config('spark.kubernetes.memoryOverheadFactor', '0.1')\n",
    "\n",
    "    spark = _spark_initialize(config=spark_config, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='1.0.0')\n",
    "    \n",
    "    output = processor.run_spark_job(fileset,\n",
    "                                     METProcessor(),\n",
    "                                     spark_executor,\n",
    "                                     spark=spark,\n",
    "                                     partitionsize=partitionsize,\n",
    "                                     thread_workers=thread_workers,\n",
    "                                     executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#@pytest.mark.skip(reason=\"Dataset is too big! no way of currently testing this...\")\n",
    "@pytest.mark.benchmark(group=\"coffea-laurelin-adl-example1\")\n",
    "@pytest.mark.parametrize(\"laurelin_version\", available_laurelin_version)\n",
    "@pytest.mark.parametrize(\"root_file\", fileset)\n",
    "def test_coffea_laurelin_adl_example1(benchmark, laurelin_version, root_file):\n",
    "    benchmark(coffea_laurelin_adl_example1, laurelin_version, fileset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
