{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.0.0/laurelin-1.0.0.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pyspark.sql\n",
    "from pyarrow.compat import guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be changed\n",
    "partitionsize = 20000\n",
    "# parameters to be changed\n",
    "thread_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'MET Masked by Jet': { 'files': ['root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root'],\n",
    "             'treename': 'Events'\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# parameters to be changed\n",
    "available_laurelin_version = [(\"edu.vanderbilt.accre:laurelin:1.0.0\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program plots an event-level variable (MET) based on conditionals with its associated Jet arrays (in this case, where at least 2 have pT > 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class JetMETProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self._columns = ['MET_pt', 'nJet', 'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_mass']\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        MET_axis = hist.Bin(\"MET_pt\", \"MET [GeV]\", 50, 0, 125)\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'MET_pt': hist.Hist(\"Counts\", dataset_axis, MET_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int)\n",
    "        })\n",
    "        \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()   \n",
    "        dataset = df[\"dataset\"]\n",
    "        jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt=df['Jet_pt'].content,\n",
    "            eta=df['Jet_eta'].content,\n",
    "            phi=df['Jet_phi'].content,\n",
    "            mass=df['Jet_mass'].content,\n",
    "            )\n",
    "        # We can access keys without appealing to a JCA, as well.\n",
    "        MET = df['MET_pt']\n",
    "        output['cutflow']['all events'] += jets.size \n",
    "        # We want jets with a pt of at least 40.\n",
    "        pt_min = (jets['p4'].pt > 40)\n",
    "        # We want MET where the above condition is met for at least two jets. The above is a list of Boolean sublists generated from the jet sublists (True if condition met, False if not). If we sum each sublist, we get the amount of jets matching the condition (since True = 1).\n",
    "        good_MET = MET[(pt_min.sum() >= 2)]\n",
    "        output['cutflow']['final events'] += good_MET.size\n",
    "        output['MET_pt'].fill(dataset=dataset, MET_pt=good_MET.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def coffea_laurelin_adl_example4(laurelin_version, fileset):\n",
    "    spark_config = pyspark.sql.SparkSession.builder \\\n",
    "        .appName('spark-executor-test-%s' % guid()) \\\n",
    "        .master('local[*]') \\\n",
    "        .config('spark.driver.memory', '4g') \\\n",
    "        .config('spark.executor.memory', '6g') \\\n",
    "        .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "        .config('spark.sql.execution.arrow.maxRecordsPerBatch', 20000)\\\n",
    "        .config('spark.driver.extraClassPath', './laurelin-1.0.0.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar')\\\n",
    "        .config('spark.kubernetes.container.image.pullPolicy', 'true')\\\n",
    "        .config('spark.kubernetes.container.image', 'gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin')\\\n",
    "        .config('spark.kubernetes.memoryOverheadFactor', '0.1')\n",
    "\n",
    "\n",
    "    spark = _spark_initialize(config=spark_config, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='1.0.0')\n",
    "    \n",
    "    output = processor.run_spark_job(fileset,\n",
    "                                     JetMETProcessor(),\n",
    "                                     spark_executor, \n",
    "                                     spark=spark,\n",
    "                                     partitionsize=partitionsize,\n",
    "                                     thread_workers=thread_workers,\n",
    "                                     executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@pytest.mark.skip(reason=\"Dataset is too big! no way of currently testing this...\")\n",
    "@pytest.mark.benchmark(group=\"coffea-laurelin-adl-example4\")\n",
    "@pytest.mark.parametrize(\"laurelin_version\", available_laurelin_version)\n",
    "@pytest.mark.parametrize(\"root_file\", fileset)\n",
    "def test_coffea_laurelin_adl_example4(benchmark, laurelin_version, root_file):\n",
    "    benchmark(coffea_laurelin_adl_example4, laurelin_version, fileset)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
