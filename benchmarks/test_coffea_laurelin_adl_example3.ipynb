{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import pyspark.sql\n",
    "from pyarrow.compat import guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be changed\n",
    "partitionsize = 200000\n",
    "# parameters to be changed\n",
    "thread_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'Jets with pT > 20 and abs(eta) < 1': { 'files': ['root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root'],\n",
    "             'treename': 'Events'\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# parameters to be changed\n",
    "available_laurelin_version = [(\"edu.vanderbilt.accre:laurelin:1.0.1-SNAPSHOT\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program plots a per-event array (jet_pt) that has been masked to meet certain conditions (in this case, abs(jet eta) < 1).\n",
    "class JetProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self._columns = ['MET_pt', 'nJet', 'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_mass']\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        Jet_axis = hist.Bin(\"Jet_pt\", \"Jet_pt [GeV]\", 100, 15, 60) \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'Jet_pt': hist.Hist(\"Counts\", dataset_axis, Jet_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int)\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = df[\"dataset\"]\n",
    "        \n",
    "        # JaggedCandidateArray bundles together keys from the TTree dict into a TLorentzVector, as well as any amount of additional keys. To refer to a TLorentzVector property, use \"JCA\"['p4'].\"property\" or \"JCA\".\"property\"; to refer to extra keys, \"JCA\"[\"property\"]\n",
    "        jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt=df['Jet_pt'].content,\n",
    "            eta=df['Jet_eta'].content,\n",
    "            phi=df['Jet_phi'].content,\n",
    "            mass=df['Jet_mass'].content,\n",
    "            )\n",
    "\n",
    "        output['cutflow']['all events'] += jets.size\n",
    "        output['cutflow']['number of jets'] += jets.counts.sum()\n",
    "        \n",
    "        # We want jets with an abs(eta) < 1. Conditionals act on every value in an array in Coffea, so this is easy. Note that we must give one conditional statement as a time. Something of the sort '1 < jets['p4'].eta < 1' WILL return an error! An alternative to below is ((jets['p4'].eta < 1) & (jets['p4'].eta > -1)), but this feels unnecessarily long when numpy can be used. Also, don't use 'and', use '&'!\n",
    "        eta_max = (np.absolute(jets['p4'].eta) < 1)\n",
    "        # eta_max is a Boolean array, with True in the place of values where the condition is met, and False otherwise. We want to sum up all the Trues (=1) in each sublist, then sum up all the sublists to get the number of jets with pt > 20.\n",
    "        output['cutflow']['abs(eta) < 1'] += eta_max.sum().sum()\n",
    "            \n",
    "        # We define good_jets as the actual jets we want to graph. We mask it with the jets that have abs(eta) < 1.\n",
    "        good_jets = jets[eta_max]\n",
    "        # good_jets is no longer a Boolean array, so we can't just sum up the True's. We count the amount of jets and sum that.\n",
    "        output['cutflow']['final good jets'] += good_jets.counts.sum()\n",
    "        \n",
    "        output['Jet_pt'].fill(dataset=dataset, Jet_pt=good_jets['p4'].pt.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def coffea_laurelin_adl_example3(laurelin_version, fileset):\n",
    "    spark_config = pyspark.sql.SparkSession.builder \\\n",
    "        .appName('spark-executor-test-%s' % guid()) \\\n",
    "        .master('local[*]') \\\n",
    "        .config('spark.driver.memory', '4g') \\\n",
    "        .config('spark.executor.memory', '4g') \\\n",
    "        .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "        .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "    spark = _spark_initialize(config=spark_config, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='1.0.1-SNAPSHOT')\n",
    "    \n",
    "    output = processor.run_spark_job(fileset,\n",
    "                                     JetProcessor(),\n",
    "                                     spark_executor,\n",
    "                                     spark=spark,\n",
    "                                     partitionsize=partitionsize,\n",
    "                                     thread_workers=thread_workers,\n",
    "                                     executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@pytest.mark.skip(reason=\"Dataset is too big! no way of currently testing this...\")\n",
    "@pytest.mark.benchmark(group=\"coffea-laurelin-adl-example3\")\n",
    "@pytest.mark.parametrize(\"laurelin_version\", available_laurelin_version)\n",
    "@pytest.mark.parametrize(\"root_file\", fileset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_coffea_laurelin_adl_example3(benchmark, laurelin_version, root_file):\n",
    "    benchmark(coffea_laurelin_adl_example3, laurelin_version, fileset)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
