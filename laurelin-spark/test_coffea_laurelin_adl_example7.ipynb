{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "# (for .py version -> next line should be commented since they are converted to ipybn via jupytext)\n",
    "!pip install --user --upgrade coffea\n",
    "# Preparation for testing\n",
    "!pip install --user --upgrade ipytest\n",
    "!pip install --user --upgrade pytest-benchmark\n",
    "!pip install --user --upgrade pytest-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.0.0/laurelin-1.0.0.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    import os\n",
    "    import ipytest\n",
    "    ipytest.config(rewrite_asserts=True, magics=True)\n",
    "    __file__ = 'test_coffea_laurelin_adl_example7.ipynb'\n",
    "    # Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "    os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ':' + '/eos/user/o/oshadura/.local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "available_laurelin_version = [(\"edu.vanderbilt.accre:laurelin:1.0.1-SNAPSHOT\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'Jets Masked by Leptons': { 'files': ['root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root'],\n",
    "             'treename': 'Events'\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# This program will graph the sum of Jet pT's which are greater than 30 GeV and farther than a Euclidean distance of 0.4 from any lepton with pT > 10 GeV.\n",
    "class JetLeptonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self._columns = ['MET_pt', 'nMuon', 'Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_mass', 'Muon_charge',\n",
    "                         'nJet', 'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_mass', 'Jet_charge', \n",
    "                         'nElectron', 'Electron_pt', 'Electron_eta', 'Electron_phi', 'Electron_mass', 'Electron_charge',\n",
    "                         ]\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        muon_axis = hist.Bin(\"Jet_pt\", \"Jet_pt [GeV]\", 100, 15, 200)   \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'Jet_pt': hist.Hist(\"Counts\", dataset_axis, muon_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int)\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()  \n",
    "        dataset = df[\"dataset\"]\n",
    "        # Unfortunately, there's two different types of leptons here, so we need to create three JCA's (one for each, one for jets)\n",
    "        muons = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nMuon'],\n",
    "            pt=df['Muon_pt'].content,\n",
    "            eta=df['Muon_eta'].content,\n",
    "            phi=df['Muon_phi'].content,\n",
    "            mass=df['Muon_mass'].content,\n",
    "            charge=df['Muon_charge'].content\n",
    "            )\n",
    "        electrons = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nElectron'],\n",
    "            pt=df['Electron_pt'].content,\n",
    "            eta=df['Electron_eta'].content,\n",
    "            phi=df['Electron_phi'].content,\n",
    "            mass=df['Electron_mass'].content,\n",
    "            charge=df['Electron_charge'].content\n",
    "            )\n",
    "        jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt=df['Jet_pt'].content,\n",
    "            eta=df['Jet_eta'].content,\n",
    "            phi=df['Jet_phi'].content,\n",
    "            mass=df['Jet_mass'].content,\n",
    "            )\n",
    "        \n",
    "        output['cutflow']['all events'] += jets.size\n",
    "        output['cutflow']['all jets'] += jets.counts.sum()\n",
    "        \n",
    "        # Get jets with higher GeV than 30.\n",
    "        min_jetpt = (jets['p4'].pt > 30)\n",
    "        output['cutflow']['jets with pt > 30'] += min_jetpt.sum().sum()\n",
    "        \n",
    "        # Get all leptons with higher GeV than 10.\n",
    "        min_muonpt = (muons['p4'].pt > 10)\n",
    "        output['cutflow']['muons with pt > 10'] += min_muonpt.sum().sum()\n",
    "        min_electronpt = (electrons['p4'].pt > 10)\n",
    "        output['cutflow']['electrons with pt > 10'] += min_electronpt.sum().sum()\n",
    "        \n",
    "        # Mask jets and leptons with their minimum requirements/\n",
    "        goodjets = jets[min_jetpt]\n",
    "        goodmuons = muons[min_muonpt]\n",
    "        goodelectrons = electrons[min_electronpt]\n",
    "        \n",
    "        # Cross is like distincts, but across multiple JCA's. So we cross jets with each lepton to generate all (jet, lepton) pairs. We have nested=True so that all jet values are stored in sublists together, and thus maintain uniqueness so we can get them back later.\n",
    "        jet_muon_pairs = goodjets['p4'].cross(goodmuons['p4'], nested=True)\n",
    "        jet_electron_pairs = goodjets['p4'].cross(goodelectrons['p4'], nested=True)\n",
    "    \n",
    "        # This long conditional checks that the jet is at least 0.4 euclidean distance from each lepton. It then checks if each unique jet contains a False, i.e., that a jet is 0.4 euclidean distance from EVERY specific lepton in the event.\n",
    "        good_jm_pairs = (jet_muon_pairs.i0.delta_r(jet_muon_pairs.i1) > 0.4).all() != False\n",
    "        good_je_pairs = (jet_electron_pairs.i0.delta_r(jet_electron_pairs.i1) > 0.4).all() != False\n",
    "        \n",
    "        output['cutflow']['jet-muon pairs'] += good_jm_pairs.sum().sum()\n",
    "        output['cutflow']['jet-electron pairs'] += good_je_pairs.sum().sum()\n",
    "        output['cutflow']['jet-lepton pairs'] += (good_jm_pairs & good_je_pairs).sum().sum()\n",
    "        \n",
    "        # We then mask our jets with all three of the above good pairs to get only jets that are 0.4 distance from every type of lepton, and sum them.\n",
    "        sumjets = goodjets['p4'][good_jm_pairs & good_je_pairs].pt.sum()\n",
    "        output['cutflow']['final jets'] += goodjets['p4'][good_jm_pairs & good_je_pairs].counts.sum()\n",
    "        output['Jet_pt'].fill(dataset=dataset, Jet_pt=sumjets.flatten())\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def coffea_laurelin_adl_example7(laurelin_version, n_workers, partition_size):\n",
    "    spark_config = pyspark.sql.SparkSession.builder \\\n",
    "        .appName('spark-executor-test-%s' % guid()) \\\n",
    "        .master('local[*]') \\\n",
    "        .config('spark.driver.memory', '4g') \\\n",
    "        .config('spark.executor.memory', '6g') \\\n",
    "        .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "        .config('spark.sql.execution.arrow.maxRecordsPerBatch', partition_size)\\\n",
    "        .config('spark.kubernetes.container.image.pullPolicy', 'true')\\\n",
    "        .config('spark.kubernetes.container.image', 'gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin')\\\n",
    "        .config('spark.kubernetes.memoryOverheadFactor', '0.1')\n",
    "\n",
    "    spark = _spark_initialize(config=spark_config, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='1.0.1-SNAPSHOT')\n",
    "    \n",
    "    output = processor.run_spark_job(fileset,\n",
    "                                     JetLeptonProcessor(),\n",
    "                                     spark_executor, \n",
    "                                     spark=spark,\n",
    "                                     partitionsize=partition_size,\n",
    "                                     thread_workers=n_workers,\n",
    "                                     executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@pytest.mark.benchmark(group=\"coffea-laurelin-adl-example7\")\n",
    "@pytest.mark.parametrize(\"laurelin_version\", available_laurelin_version)\n",
    "@pytest.mark.parametrize(\"n_workers\", range(1,psutil.cpu_count(logical=False)))\n",
    "@pytest.mark.parametrize(\"partition_size\", range(100000,200000,100000))\n",
    "def test_coffea_laurelin_adl_example7(benchmark,laurelin_version, n_workers, partition_size):\n",
    "    benchmark(coffea_laurelin_adl_example7, available_laurelin_version, n_workers, partition_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    ipytest.run('-qq')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
