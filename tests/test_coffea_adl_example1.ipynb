{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed\n",
    "# (e.g. on SWAN with LCG 96Python3 stack)\n",
    "# (for .py version: next line should be commented\n",
    "# since they are converted to ipybn via jupytext)\n",
    "!pip install --user --upgrade coffea awkward numba\n",
    "# Preparation for testing\n",
    "!pip install --user --upgrade ipytest pytest-benchmark pytest-csv\n",
    "# !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single-machine scheduler:\n",
    "# https://docs.dask.org/en/latest/setup.html\n",
    "# https://docs.dask.org/en/latest/setup/single-machine.html\n",
    "! pip install --user dask distributed dask-jobqueue blosc --upgrades\n",
    "! pip install --force-reinstall --ignore-installed git+git://github.com/oshadura/distributed.git@coffea-casa-facility#egg=distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to test Dask on UNL HTCondor:  %env DASK_COFFEABENCH_SETUP=\"unl-htcondor\"\n",
    "# Uncomment this if you want to test Dask on UNL AF coffea-casa: %env DASK_COFFEABENCH_SETUP=\"coffea-casa\"\n",
    "# Uncomment this if you want to test Dask locally:  %env DASK_COFFEABENCH_SETUP=\"local\"\n",
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.1.1/laurelin-1.1.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to test Dask:  %env DASK_COFFEABENCH=1\n",
    "# Uncomment this if you want to test Spark: %env PYSPARK_COFFEABENCH=1\n",
    "# Uncomment this if you want to test uproot:  %env UPROOT_COFFEABENCH=1\n",
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import psutil\n",
    "import pytest\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from uproot_methods import TLorentzVectorArray\n",
    "import coffea.processor as processor\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    import os\n",
    "    import ipytest\n",
    "\n",
    "    ipytest.config(rewrite_asserts=True, magics=True)\n",
    "    __file__ = \"test_coffea_adl_example8.ipynb\"\n",
    "    # Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "    os.environ[\"PYTHONPATH\"] = (\n",
    "        os.environ[\"PYTHONPATH\"] + \":\" + \"/usr/local/lib/python3.6/site-packages\"\n",
    "    )\n",
    "    os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/eos/user/o/oshadura/.local/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PYSPARK_COFFEABENCH\" in os.environ and os.environ[\"PYSPARK_COFFEABENCH\"] == \"1\":\n",
    "    import pyspark.sql\n",
    "    from pyarrow.compat import guid\n",
    "    from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "    from coffea.processor.spark.spark_executor import spark_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_laurelin_version = [(\"edu.vanderbilt.accre:laurelin:1.1.1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DASK_COFFEABENCH\" in os.environ and os.environ[\"DASK_COFFEABENCH\"] == \"1\":\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    from dask_jobqueue import HTCondorCluster\n",
    "    from coffea_casa import CoffeaCasaCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    \"SingleMu\": [\n",
    "        \"root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The processor class bundles our data analysis together while giving us some\n",
    "# helpful tools.  It also leaves looping and chunks to the framework instead of us.\n",
    "class Processor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        # Bins and categories for the histogram are defined here. For format,\n",
    "        # see https://coffeateam.github.io/coffea/stubs/coffea.hist.hist_tools.Hist.html && https://coffeateam.github.io/coffea/stubs/coffea.hist.hist_tools.Bin.html\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        MET_axis = hist.Bin(\"MET\", \"MET [GeV]\", 50, 0, 100)\n",
    "\n",
    "        # The accumulator keeps our data chunks together for histogramming.\n",
    "        # It also gives us cutflow, which can be used to keep track of data.\n",
    "        self._accumulator = processor.dict_accumulator(\n",
    "            {\n",
    "                \"MET\": hist.Hist(\"Counts\", dataset_axis, MET_axis),\n",
    "                \"cutflow\": processor.defaultdict_accumulator(int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        # This is where we do our actual analysis. The dataset has columns\n",
    "        # similar to the TTree's; events.columns can tell you them, or events.\n",
    "        # [object].columns for deeper depth.\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        MET = events.MET.pt\n",
    "\n",
    "        # We can define a new key for cutflow (in this case 'all events').\n",
    "        # Then we can put values into it. We need += because it's per-chunk (demonstrated below)\n",
    "        output[\"cutflow\"][\"all events\"] += MET.size\n",
    "        output[\"cutflow\"][\"number of chunks\"] += 1\n",
    "\n",
    "        # This fills our histogram once our data is collected.\n",
    "        # The hist key ('MET=') will be defined in the bin in __init__.\n",
    "        output[\"MET\"].fill(dataset=dataset, MET=MET.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DASK_COFFEABENCH\" in os.environ and os.environ[\"DASK_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    def test_dask_adlexample1(benchmark):\n",
    "        @benchmark\n",
    "        def dask_adlexample1(n_cores=2):\n",
    "            # Dask settings (three different cases)\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"unl-tier3\":\n",
    "                client = Client(\"t3.unl.edu:8786\")\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"coffea-casa\":\n",
    "                cluster = CoffeaCasaCluster()\n",
    "                cluster.scale(jobs=5)\n",
    "                client = Client(cluster)\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"local\":\n",
    "                client = Client()\n",
    "            exe_args = {\"client\": client, \"nano\": True}\n",
    "            output = processor.run_uproot_job(\n",
    "                fileset,\n",
    "                treename=\"Events\",\n",
    "                processor_instance=Processor(),\n",
    "                executor=processor.dask_executor,\n",
    "                executor_args=exe_args,\n",
    "            )\n",
    "            hist.plot1d(\n",
    "                output[\"MET\"],\n",
    "                overlay=\"dataset\",\n",
    "                fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "            )\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coffea_laurelin_adlexample1(laurelin_version, n_workers, partition_size):\n",
    "    spark_config = (\n",
    "        pyspark.sql.SparkSession.builder.appName(\"spark-executor-test-%s\" % guid())\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", partition_size)\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"true\")\n",
    "        .config(\n",
    "            \"spark.kubernetes.container.image\",\n",
    "            \"gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin\",\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.driver.extraClassPath\",\n",
    "            \"./laurelin-1.0.0.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar\",\n",
    "        )\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.1\")\n",
    "    )\n",
    "    spark = _spark_initialize(\n",
    "        config=spark_config,\n",
    "        log_level=\"WARN\",\n",
    "        spark_progress=False,\n",
    "        laurelin_version=\"1.1.1-SNAPSHOT\",\n",
    "    )\n",
    "    output = processor.run_spark_job(\n",
    "        fileset,\n",
    "        Processor(),\n",
    "        spark_executor,\n",
    "        spark=spark,\n",
    "        partitionsize=partition_size,\n",
    "        thread_workers=n_workers,\n",
    "        executor_args={\n",
    "            \"file_type\": \"edu.vanderbilt.accre.laurelin.Root\",\n",
    "            \"cache\": False,\n",
    "        },\n",
    "    )\n",
    "    hist.plot1d(\n",
    "        output[\"MET\"],\n",
    "        overlay=\"dataset\",\n",
    "        fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PYSPARK_COFFEABENCH\" in os.environ and os.environ[\"PYSPARK_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    @pytest.mark.benchmark(group=\"coffea-laurelin-adl-example1\")\n",
    "    @pytest.mark.parametrize(\"laurelin_version\", available_laurelin_version)\n",
    "    @pytest.mark.parametrize(\"n_workers\", range(1, psutil.cpu_count(logical=False)))\n",
    "    @pytest.mark.parametrize(\"partition_size\", range(200000, 500000, 200000))\n",
    "    def test_laurelin_adlexample1(\n",
    "        benchmark, laurelin_version, n_workers, partition_size\n",
    "    ):\n",
    "        benchmark(\n",
    "            coffea_laurelin_adlexample1,\n",
    "            available_laurelin_version,\n",
    "            n_workers,\n",
    "            partition_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coffea_uproot_adlexample1(n_workers, chunk_size):\n",
    "    output = processor.run_uproot_job(\n",
    "        fileset,\n",
    "        treename=\"Events\",\n",
    "        processor_instance=Processor(),\n",
    "        executor=processor.futures_executor,\n",
    "        chunksize=chunk_size,\n",
    "        executor_args={\"workers\": n_workers, \"nano\": True},\n",
    "    )\n",
    "    hist.plot1d(\n",
    "        output[\"MET\"],\n",
    "        overlay=\"dataset\",\n",
    "        fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if \"UPROOT_COFFEABENCH\" in os.environ and os.environ[\"UPROOT_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    @pytest.mark.parametrize(\"n_workers\", range(1, psutil.cpu_count(logical=False)))\n",
    "    @pytest.mark.parametrize(\"chunk_size\", range(200000, 500000, 200000))\n",
    "    def test_coffea_uproot_adl_example8(benchmark, n_workers, chunk_size):\n",
    "        benchmark(coffea_uproot_adlexample1, n_workers, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    ipytest.run(\"-qq\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
