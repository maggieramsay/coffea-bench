{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed\n",
    "# (e.g. on SWAN with LCG 96Python3 stack)\n",
    "# (for .py version: next line should be commented\n",
    "# since they are converted to ipybn via jupytext)\n",
    "!pip install --user --upgrade coffea awkward numba\n",
    "# Preparation for testing\n",
    "!pip install --user --upgrade ipytest pytest-benchmark pytest-csv\n",
    "# !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single-machine scheduler:\n",
    "# https://docs.dask.org/en/latest/setup.html\n",
    "# https://docs.dask.org/en/latest/setup/single-machine.html\n",
    "! pip install --user dask distributed dask-jobqueue blosc --upgrades\n",
    "! pip install --force-reinstall --ignore-installed git+git://github.com/oshadura/distributed.git@coffea-casa-facility#egg=distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to test Dask on UNL HTCondor:  %env DASK_COFFEABENCH_SETUP=\"unl-htcondor\"\n",
    "# Uncomment this if you want to test Dask on UNL AF coffea-casa: %env DASK_COFFEABENCH_SETUP=\"coffea-casa\"\n",
    "# Uncomment this if you want to test Dask locally:  %env DASK_COFFEABENCH_SETUP=\"local\"\n",
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.1.1/laurelin-1.1.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to test Dask:  %env DASK_COFFEABENCH=1\n",
    "# Uncomment this if you want to test Spark: %env PYSPARK_COFFEABENCH=1\n",
    "# Uncomment this if you want to test uproot:  %env UPROOT_COFFEABENCH=1\n",
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    import os\n",
    "    import ipytest\n",
    "\n",
    "    ipytest.config(rewrite_asserts=True, magics=True)\n",
    "    __file__ = \"test_coffea_adl_example7.ipynb\"\n",
    "    # Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "    os.environ[\"PYTHONPATH\"] = (\n",
    "        os.environ[\"PYTHONPATH\"] + \":\" + \"/usr/local/lib/python3.6/site-packages\"\n",
    "    )\n",
    "    os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/eos/user/o/oshadura/.local/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import psutil\n",
    "import pytest\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from uproot_methods import TLorentzVectorArray\n",
    "import coffea.processor as processor\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    import os\n",
    "    import ipytest\n",
    "\n",
    "    ipytest.config(rewrite_asserts=True, magics=True)\n",
    "    __file__ = \"test_coffea_adl_example8.ipynb\"\n",
    "    # Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "    os.environ[\"PYTHONPATH\"] = (\n",
    "        os.environ[\"PYTHONPATH\"] + \":\" + \"/usr/local/lib/python3.6/site-packages\"\n",
    "    )\n",
    "    os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/eos/user/o/oshadura/.local/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PYSPARK_COFFEABENCH\" in os.environ and os.environ[\"PYSPARK_COFFEABENCH\"] == \"1\":\n",
    "    import pyspark.sql\n",
    "    from pyarrow.compat import guid\n",
    "    from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "    from coffea.processor.spark.spark_executor import spark_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_laurelin_version = [(\"edu.vanderbilt.accre:laurelin:1.1.1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DASK_COFFEABENCH\" in os.environ and os.environ[\"DASK_COFFEABENCH\"] == \"1\":\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    from dask_jobqueue import HTCondorCluster\n",
    "    from coffea_casa import CoffeaCasaCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    \"SingleMu\": [\n",
    "        \"root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program will graph the sum of Jet pT's which are greater than 30 GeV and\n",
    "# farther than a Euclidean distance of 0.4 from any lepton with pT > 10 GeV.\n",
    "class Processor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        muon_axis = hist.Bin(\"Jet_pt\", \"Jet_pt [GeV]\", 100, 15, 200)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator(\n",
    "            {\n",
    "                \"Jet_pt\": hist.Hist(\"Counts\", dataset_axis, muon_axis),\n",
    "                \"cutflow\": processor.defaultdict_accumulator(int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        muons = events.Muon\n",
    "        electrons = events.Electron\n",
    "        jets = events.Jet\n",
    "        output[\"cutflow\"][\"all events\"] += jets.size\n",
    "        output[\"cutflow\"][\"all jets\"] += jets.counts.sum()\n",
    "        # Get jets with higher GeV than 30.\n",
    "        min_jetpt = jets.pt > 30\n",
    "        output[\"cutflow\"][\"jets with pt > 30\"] += min_jetpt.sum().sum()\n",
    "        # Get all leptons with higher GeV than 10.\n",
    "        min_muonpt = muons.pt > 10\n",
    "        output[\"cutflow\"][\"muons with pt > 10\"] += min_muonpt.sum().sum()\n",
    "        min_electronpt = electrons.pt > 10\n",
    "        output[\"cutflow\"][\"electrons with pt > 10\"] += min_electronpt.sum().sum()\n",
    "        # Mask jets and leptons with their minimum requirements/\n",
    "        goodjets = jets[min_jetpt]\n",
    "        goodmuons = muons[min_muonpt]\n",
    "        goodelectrons = electrons[min_electronpt]\n",
    "        # Cross is like distincts, but across multiple JCA's.\n",
    "        # So we cross jets with each lepton to generate all (jet, lepton) pairs.\n",
    "        # We have nested=True so that all jet values are stored in sublists together\n",
    "        # and thus maintain uniqueness so we can get them back later.\n",
    "        jet_muon_pairs = goodjets.cross(goodmuons, nested=True)\n",
    "        jet_electron_pairs = goodjets.cross(goodelectrons, nested=True)\n",
    "        # This long conditional checks that the jet is at least 0.4 euclidean\n",
    "        # distance from each lepton. It then checks if each unique jet contains\n",
    "        # a False, i.e., that a jet is 0.4 euclidean distance from EVERY specific lepton in the event.\n",
    "        good_jm_pairs = (jet_muon_pairs.i0.delta_r(jet_muon_pairs.i1) > 0.4).all()\n",
    "        good_je_pairs = (\n",
    "            jet_electron_pairs.i0.delta_r(jet_electron_pairs.i1) > 0.4\n",
    "        ).all()\n",
    "        output[\"cutflow\"][\"jet-muon pairs\"] += good_jm_pairs.sum().sum()\n",
    "        output[\"cutflow\"][\"jet-electron pairs\"] += good_je_pairs.sum().sum()\n",
    "        output[\"cutflow\"][\"jet-lepton pairs\"] += (\n",
    "            (good_jm_pairs & good_je_pairs).sum().sum()\n",
    "        )\n",
    "        # We then mask our jets with all three of the above good pairs to get\n",
    "        # only jets that are 0.4 distance from every type of lepton, and sum them.\n",
    "        sumjets = goodjets[good_jm_pairs & good_je_pairs].pt.sum()\n",
    "        output[\"cutflow\"][\"final jets\"] += goodjets[\n",
    "            good_jm_pairs & good_je_pairs\n",
    "        ].counts.sum()\n",
    "        output[\"Jet_pt\"].fill(dataset=dataset, Jet_pt=sumjets.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DASK_COFFEABENCH\" in os.environ and os.environ[\"DASK_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    def test_dask_adlexample7(benchmark):\n",
    "        @benchmark\n",
    "        def dask_adlexample7(n_cores=2):\n",
    "            # Dask settings (three different cases)\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"unl-tier3\":\n",
    "                client = Client(\"t3.unl.edu:8786\")\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"coffea-casa\":\n",
    "                cluster = CoffeaCasaCluster()\n",
    "                cluster.scale(jobs=5)\n",
    "                client = Client(cluster)\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"local\":\n",
    "                client = Client()\n",
    "            exe_args = {\"client\": client, \"nano\": True}\n",
    "            output = processor.run_uproot_job(\n",
    "                fileset,\n",
    "                treename=\"Events\",\n",
    "                processor_instance=Processor(),\n",
    "                executor=processor.dask_executor,\n",
    "                executor_args=exe_args,\n",
    "            )\n",
    "            hist.plot1d(\n",
    "                output[\"MET\"],\n",
    "                overlay=\"dataset\",\n",
    "                fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "            )\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coffea_laurelin_adl_example7(laurelin_version, n_workers, partition_size):\n",
    "    spark_config = (\n",
    "        pyspark.sql.SparkSession.builder.appName(\"spark-executor-test-%s\" % guid())\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", partition_size)\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"true\")\n",
    "        .config(\n",
    "            \"spark.kubernetes.container.image\",\n",
    "            \"gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin\",\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.driver.extraClassPath\",\n",
    "            \"./laurelin-1.0.0.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar\",\n",
    "        )\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.1\")\n",
    "    )\n",
    "    spark = _spark_initialize(\n",
    "        config=spark_config,\n",
    "        log_level=\"WARN\",\n",
    "        spark_progress=False,\n",
    "        laurelin_version=\"1.1.1-SNAPSHOT\",\n",
    "    )\n",
    "    output = processor.run_spark_job(\n",
    "        fileset,\n",
    "        Processor(),\n",
    "        spark_executor,\n",
    "        spark=spark,\n",
    "        partitionsize=partition_size,\n",
    "        thread_workers=n_workers,\n",
    "        executor_args={\n",
    "            \"file_type\": \"edu.vanderbilt.accre.laurelin.Root\",\n",
    "            \"cache\": False,\n",
    "        },\n",
    "    )\n",
    "    hist.plot1d(\n",
    "        output[\"Jet_pt\"],\n",
    "        overlay=\"dataset\",\n",
    "        fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PYSPARK_COFFEABENCH\" in os.environ and os.environ[\"PYSPARK_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    @pytest.mark.benchmark(group=\"coffea-laurelin-adl-example7\")\n",
    "    @pytest.mark.parametrize(\"laurelin_version\", available_laurelin_version)\n",
    "    @pytest.mark.parametrize(\"n_workers\", range(1, psutil.cpu_count(logical=False)))\n",
    "    @pytest.mark.parametrize(\"partition_size\", range(200000, 500000, 200000))\n",
    "    def test_coffea_laurelin_adl_example7(\n",
    "        benchmark, laurelin_version, n_workers, partition_size\n",
    "    ):\n",
    "        benchmark(\n",
    "            coffea_laurelin_adl_example7,\n",
    "            available_laurelin_version,\n",
    "            n_workers,\n",
    "            partition_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coffea_uproot_adl_example7(n_workers, chunk_size):\n",
    "    output = processor.run_uproot_job(\n",
    "        fileset,\n",
    "        treename=\"Events\",\n",
    "        processor_instance=Processor(),\n",
    "        executor=processor.futures_executor,\n",
    "        chunksize=chunk_size,\n",
    "        executor_args={\"workers\": n_workers, \"nano\": True},\n",
    "    )\n",
    "    hist.plot1d(\n",
    "        output[\"Jet_pt\"],\n",
    "        overlay=\"dataset\",\n",
    "        fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if \"UPROOT_COFFEABENCH\" in os.environ and os.environ[\"UPROOT_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    @pytest.mark.benchmark(group=\"coffea-uproot-adl-example7\")\n",
    "    @pytest.mark.parametrize(\"n_workers\", range(1, psutil.cpu_count(logical=False)))\n",
    "    @pytest.mark.parametrize(\"chunk_size\", range(200000, 500000, 200000))\n",
    "    def test_coffea_uproot_adl_example7(\n",
    "        benchmark, n_workers, chunk_size, maxchunk_size\n",
    "    ):\n",
    "        benchmark(coffea_uproot_adl_example7, n_workers, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    ipytest.run(\"-qq\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
