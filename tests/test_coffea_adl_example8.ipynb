{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed\n",
    "# (e.g. on SWAN with LCG 96Python3 stack)\n",
    "# (for .py version: next line should be commented\n",
    "# since they are converted to ipybn via jupytext)\n",
    "!pip install --user --upgrade coffea awkward numba\n",
    "# Preparation for testing\n",
    "!pip install --user --upgrade ipytest pytest-benchmark pytest-csv\n",
    "# !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single-machine scheduler:\n",
    "# https://docs.dask.org/en/latest/setup.html\n",
    "# https://docs.dask.org/en/latest/setup/single-machine.html\n",
    "! pip install --user dask distributed dask-jobqueue blosc --upgrades\n",
    "! pip install --force-reinstall --ignore-installed git+git://github.com/oshadura/distributed.git@coffea-casa-facility#egg=distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to test Dask on UNL HTCondor:  %env DASK_COFFEABENCH_SETUP=\"unl-htcondor\"\n",
    "# Uncomment this if you want to test Dask on UNL AF coffea-casa: %env DASK_COFFEABENCH_SETUP=\"coffea-casa\"\n",
    "# Uncomment this if you want to test Dask locally:  %env DASK_COFFEABENCH_SETUP=\"local\"\n",
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.1.1/laurelin-1.1.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to test Dask:  %env DASK_COFFEABENCH=1\n",
    "# Uncomment this if you want to test Spark: %env PYSPARK_COFFEABENCH=1\n",
    "# Uncomment this if you want to test uproot:  %env UPROOT_COFFEABENCH=1\n",
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import psutil\n",
    "import pytest\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from uproot_methods import TLorentzVectorArray\n",
    "import coffea.processor as processor\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    import os\n",
    "    import ipytest\n",
    "\n",
    "    ipytest.config(rewrite_asserts=True, magics=True)\n",
    "    __file__ = \"test_coffea_adl_example8.ipynb\"\n",
    "    # Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "    os.environ[\"PYTHONPATH\"] = (\n",
    "        os.environ[\"PYTHONPATH\"] + \":\" + \"/usr/local/lib/python3.6/site-packages\"\n",
    "    )\n",
    "    os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + \"/eos/user/o/oshadura/.local/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PYSPARK_COFFEABENCH\" in os.environ and os.environ[\"PYSPARK_COFFEABENCH\"] == \"1\":\n",
    "    import pyspark.sql\n",
    "    from pyarrow.compat import guid\n",
    "    from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "    from coffea.processor.spark.spark_executor import spark_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_laurelin_version = [(\"edu.vanderbilt.accre:laurelin:1.1.1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DASK_COFFEABENCH\" in os.environ and os.environ[\"DASK_COFFEABENCH\"] == \"1\":\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    from dask_jobqueue import HTCondorCluster\n",
    "    from coffea_casa import CoffeaCasaCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    \"SingleMu\": [\n",
    "        \"root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root\"\n",
    "    ]\n",
    "}\n",
    "# This is a helper function which adds muon (0)\n",
    "# and electron (1) identifiers to a stacked lepton JaggedArray.\n",
    "def make_labeled_p4(x, indices, itype):\n",
    "    p4 = TLorentzVectorArray.from_ptetaphim(x.pt, x.eta, x.phi, x.mass)\n",
    "    return ak.JaggedArray.zip(\n",
    "        p4=p4,\n",
    "        ptype=itype * x.pt.ones_like().astype(np.int),\n",
    "        flavor=indices,\n",
    "        charge=x.charge,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_leptons(muons, electrons):\n",
    "    \"\"\"\n",
    "    # This generates a stacked lepton JaggedArray,\n",
    "    allowing combination of both muons and electrons for computations across flavor.\n",
    "    \"\"\"\n",
    "    # Construct new lepton indices within every event array.\n",
    "    muons_indices = (\n",
    "        ak.JaggedArray.fromoffsets(\n",
    "            muons.pt.offsets, np.arange(0, muons.pt.content.size)\n",
    "        )\n",
    "        - muons.pt.offsets[:-1]\n",
    "    )\n",
    "    electrons_indices = (\n",
    "        ak.JaggedArray.fromoffsets(\n",
    "            electrons.pt.offsets, np.arange(0, electrons.pt.content.size)\n",
    "        )\n",
    "        - electrons.pt.offsets[:-1]\n",
    "    )\n",
    "    # Assign 0/1 value depending on whether lepton is muon/electron.\n",
    "    muons_p4 = make_labeled_p4(muons, muons_indices, 0)\n",
    "    electrons_p4 = make_labeled_p4(electrons, electrons_indices, 1)\n",
    "    # Concatenate leptons.\n",
    "    stacked_p4 = ak.concatenate((muons_p4, electrons_p4), axis=1)\n",
    "\n",
    "    return stacked_p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program plots the transverse mass of MET and a third lepton,\n",
    "# where the third lepton is associated with a lepton pair\n",
    "# that has the same flavor, opposite charge, and closest mass to 91.2.\n",
    "class Processor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"MET and Third Lepton\")\n",
    "        muon_axis = hist.Bin(\"massT\", \"Transverse Mass\", 50, 15, 250)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator(\n",
    "            {\n",
    "                \"massT\": hist.Hist(\"Counts\", dataset_axis, muon_axis),\n",
    "                \"cutflow\": processor.defaultdict_accumulator(int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        muons = events.Muon\n",
    "        electrons = events.Electron\n",
    "        MET = events.MET\n",
    "        # A few reasonable muon and electron selection cuts\n",
    "        muons = muons[(muons.pt > 10) & (np.abs(muons.eta) < 2.4)]\n",
    "        electrons = electrons[(electrons.pt > 10) & (np.abs(electrons.eta) < 2.5)]\n",
    "        leptons = stack_leptons(muons, electrons)\n",
    "        # Filter out events with less than 3 leptons.\n",
    "        MET = MET[leptons.counts >= 3]\n",
    "        trileptons = leptons[leptons.counts >= 3]\n",
    "        # Generate the indices of every pair;\n",
    "        # indices because we'll be removing these elements later.\n",
    "        lepton_pairs = trileptons.argchoose(2)\n",
    "        # Select pairs that are SFOS.\n",
    "        SFOS_pairs = lepton_pairs[\n",
    "            (trileptons[lepton_pairs.i0].flavor == trileptons[lepton_pairs.i1].flavor)\n",
    "            & (trileptons[lepton_pairs.i0].charge != trileptons[lepton_pairs.i1].charge)\n",
    "        ]\n",
    "        # Find the pair with mass closest to Z.\n",
    "        closest_pairs = SFOS_pairs[\n",
    "            np.abs(\n",
    "                (trileptons[SFOS_pairs.i0].p4 + trileptons[SFOS_pairs.i1].p4).mass\n",
    "                - 91.2\n",
    "            ).argmin()\n",
    "        ]\n",
    "        # Remove elements of these pairs from leptons by negating the indices.\n",
    "        is_in_pair_mask = trileptons[~closest_pairs.i0 | ~closest_pairs.i1]\n",
    "        # Find the highest-pt lepton out of the ones that remain.\n",
    "        leading_lepton = trileptons[trileptons.p4.pt.argmax()]\n",
    "        # Can't cross MET with leading_lepton, but we need both phi and pt. So we build a crossable table.\n",
    "        MET_tab = ak.JaggedArray.fromcounts(\n",
    "            np.ones_like(MET.pt, dtype=np.int), ak.Table({\"phi\": MET.phi, \"pt\": MET.pt})\n",
    "        )\n",
    "        met_plus_lep = MET_tab.cross(leading_lepton)\n",
    "        # Do some math to get what we want.\n",
    "        dphi_met_lep = (met_plus_lep.i0.phi - met_plus_lep.i1.p4.phi + math.pi) % (\n",
    "            2 * math.pi\n",
    "        ) - math.pi\n",
    "        mt_lep = np.sqrt(\n",
    "            2.0\n",
    "            * met_plus_lep.i0.pt\n",
    "            * met_plus_lep.i1.p4.pt\n",
    "            * (1.0 - np.cos(dphi_met_lep))\n",
    "        )\n",
    "        output[\"massT\"].fill(dataset=dataset, massT=mt_lep.flatten())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"DASK_COFFEABENCH\" in os.environ and os.environ[\"DASK_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    def test_dask_adlexample8(benchmark):\n",
    "        @benchmark\n",
    "        def dask_adlexample8(n_cores=2):\n",
    "            # Dask settings (three different cases)\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"unl-tier3\":\n",
    "                client = Client(\"t3.unl.edu:8786\")\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"coffea-casa\":\n",
    "                cluster = CoffeaCasaCluster()\n",
    "                cluster.scale(jobs=5)\n",
    "                client = Client(cluster)\n",
    "            if os.environ[\"DASK_COFFEABENCH_SETUP\"] == \"local\":\n",
    "                client = Client()\n",
    "            exe_args = {\"client\": client, \"nano\": True}\n",
    "            output = processor.run_uproot_job(\n",
    "                fileset,\n",
    "                treename=\"Events\",\n",
    "                processor_instance=Processor(),\n",
    "                executor=processor.dask_executor,\n",
    "                executor_args=exe_args,\n",
    "            )\n",
    "            hist.plot1d(\n",
    "                output[\"MET\"],\n",
    "                overlay=\"dataset\",\n",
    "                fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "            )\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coffea_laurelin_adl_example8(laurelin_version, n_workers, partition_size):\n",
    "    spark_config = (\n",
    "        pyspark.sql.SparkSession.builder.appName(\"spark-executor-test-%s\" % guid())\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .config(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", partition_size)\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"true\")\n",
    "        .config(\n",
    "            \"spark.kubernetes.container.image\",\n",
    "            \"gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin\",\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.driver.extraClassPath\",\n",
    "            \"./laurelin-1.0.0.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar\",\n",
    "        )\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.1\")\n",
    "    )\n",
    "    spark = _spark_initialize(\n",
    "        config=spark_config,\n",
    "        log_level=\"WARN\",\n",
    "        spark_progress=False,\n",
    "        laurelin_version=\"1.1.1-SNAPSHOT\",\n",
    "    )\n",
    "    output = processor.run_spark_job(\n",
    "        fileset,\n",
    "        Processor(),\n",
    "        spark_executor,\n",
    "        spark=spark,\n",
    "        partitionsize=partition_size,\n",
    "        thread_workers=n_workers,\n",
    "        executor_args={\n",
    "            \"file_type\": \"edu.vanderbilt.accre.laurelin.Root\",\n",
    "            \"cache\": False,\n",
    "        },\n",
    "    )\n",
    "    hist.plot1d(\n",
    "        output[\"massT\"],\n",
    "        overlay=\"dataset\",\n",
    "        fill_opts={\"edgecolor\": (0, 0, 0, 0.3), \"alpha\": 0.8},\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PYSPARK_COFFEABENCH\" in os.environ and os.environ[\"PYSPARK_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    @pytest.mark.benchmark(group=\"coffea-laurelin-adl-example8\")\n",
    "    @pytest.mark.parametrize(\"laurelin_version\", available_laurelin_version)\n",
    "    @pytest.mark.parametrize(\"n_workers\", range(1, psutil.cpu_count(logical=False)))\n",
    "    @pytest.mark.parametrize(\"partition_size\", range(200000, 500000, 200000))\n",
    "    def test_coffea_laurelin_adl_example8(\n",
    "        benchmark, laurelin_version, n_workers, partition_size\n",
    "    ):\n",
    "        benchmark(\n",
    "            coffea_laurelin_adl_example8,\n",
    "            available_laurelin_version,\n",
    "            n_workers,\n",
    "            partition_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coffea_uproot_adl_example8(n_workers, chunk_size):\n",
    "    output = processor.run_uproot_job(\n",
    "        fileset,\n",
    "        treename=\"Events\",\n",
    "        processor_instance=Processor(),\n",
    "        executor=processor.futures_executor,\n",
    "        chunksize=chunk_size,\n",
    "        executor_args={\"workers\": n_workers, \"nano\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if \"UPROOT_COFFEABENCH\" in os.environ and os.environ[\"UPROOT_COFFEABENCH\"] == \"1\":\n",
    "\n",
    "    @pytest.mark.benchmark(group=\"coffea-uproot-adl-example8\")\n",
    "    @pytest.mark.parametrize(\"n_workers\", range(1, psutil.cpu_count(logical=False)))\n",
    "    @pytest.mark.parametrize(\"chunk_size\", range(200000, 500000, 200000))\n",
    "    def test_coffea_uproot_adl_example8(benchmark, n_workers, chunk_size):\n",
    "        benchmark(coffea_uproot_adl_example8, n_workers, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    ipytest.run(\"-qq\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
