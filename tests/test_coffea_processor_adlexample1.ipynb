{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea-Casa Benchmark Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "%matplotlib inline\n",
    "from coffea import hist\n",
    "import coffea.processor as processor\n",
    "import awkward as ak\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import time\n",
    "import os\n",
    "import ipytest\n",
    "\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "__file__ = \"test_coffea_processor_adlexample1.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/oshadura/CERN_sources/distributed/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51516 instead\n",
      "  warnings.warn(\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "#client = Client(\"tls://localhost:8786\")\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster) \n",
    "\n",
    "fileset = {'SingleMu' : [\"root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program plots an event-level variable (in this case, MET, but switching it is as easy as a dict-key change). It also demonstrates an easy use of the book-keeping cutflow tool, to keep track of the number of events processed.\n",
    "\n",
    "# The processor class bundles our data analysis together while giving us some helpful tools.  It also leaves looping and chunks to the framework instead of us.\n",
    "class Processor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        # Bins and categories for the histogram are defined here. For format, see https://coffeateam.github.io/coffea/stubs/coffea.hist.hist_tools.Hist.html && https://coffeateam.github.io/coffea/stubs/coffea.hist.hist_tools.Bin.html\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        MET_axis = hist.Bin(\"MET\", \"MET [GeV]\", 50, 0, 100)\n",
    "        \n",
    "        # The accumulator keeps our data chunks together for histogramming. It also gives us cutflow, which can be used to keep track of data.\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'MET': hist.Hist(\"Counts\", dataset_axis, MET_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int)\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, events):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        # This is where we do our actual analysis. The dataset has columns similar to the TTree's; events.columns can tell you them, or events.[object].columns for deeper depth.\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        MET = events.MET.pt\n",
    "        \n",
    "        # We can define a new key for cutflow (in this case 'all events'). Then we can put values into it. We need += because it's per-chunk (demonstrated below)\n",
    "        output['cutflow']['all events'] += ak.size(MET)\n",
    "        output['cutflow']['number of chunks'] += 1\n",
    "        \n",
    "        # This fills our histogram once our data is collected. The hist key ('MET=') will be defined in the bin in __init__.\n",
    "        output['MET'].fill(dataset=dataset, MET=MET)\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "expression cannot contain assignment, perhaps you meant \"==\"? (<ipython-input-34-91618eb23bad>, line 7)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-91618eb23bad>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    executor_args = {'schema': processor.NanoAODSchema, 'client' = client}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"
     ]
    }
   ],
   "source": [
    "def coffea_processor_adlexample1(chunk_size):\n",
    "  output = processor.run_uproot_job(fileset,\n",
    "                                  treename = 'Events',\n",
    "                                  processor_instance = Processor(),\n",
    "                                  executor = processor.dask_executor,\n",
    "                                  chunksize = chunk_size,\n",
    "                                  executor_args = {'schema': processor.NanoAODSchema, 'client': client}\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a 1D histogram from the data output to the 'MET' key. fill_opts are optional, to fill the graph (default is a line).\n",
    "def coffea_plot_adlexample1():\n",
    "    hist.plot1d(output['MET'], overlay='dataset', fill_opts={'edgecolor': (0,0,0,0.3), 'alpha': 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "  @pytest.mark.parametrize(\"chunk_size\", range(100000, 500000, 100000))\n",
    "  def test_coffea_processor_adlexample1(benchmark, chunk_size):\n",
    "        benchmark(coffea_processor_adlexample1, chunk_size)\n",
    "        coffea_plot_adlexample1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FFFF                                                                     [100%]\n",
      "=================================== FAILURES ===================================\n",
      "__________________ test_coffea_processor_adlexample1[100000] ___________________\n",
      "\n",
      "benchmark = <pytest_benchmark.fixture.BenchmarkFixture object at 0x7fdca03a4a90>\n",
      "chunk_size = 100000\n",
      "\n",
      "    @pytest.mark.parametrize(\"chunk_size\", range(100000, 500000, 100000))\n",
      "    def test_coffea_processor_adlexample1(benchmark, chunk_size):\n",
      ">         benchmark(coffea_processor_adlexample1, chunk_size)\n",
      "\n",
      "<ipython-input-29-62d1ae0f702f>:3: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:127: in __call__\n",
      "    return self._raw(function_to_benchmark, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:149: in _raw\n",
      "    duration, iterations, loops_range = self._calibrate_timer(runner)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:277: in _calibrate_timer\n",
      "    duration = runner(loops_range)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:92: in runner\n",
      "    function_to_benchmark(*args, **kwargs)\n",
      "<ipython-input-27-255f1a1946c1>:2: in coffea_processor_adlexample1\n",
      "    output = processor.run_uproot_job(fileset,\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:1385: in run_uproot_job\n",
      "    out = pre_executor(to_get, metadata_fetcher, out, **pre_args)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "items = {<coffea.processor.executor.FileMeta object at 0x7fdca0341180>}\n",
      "function = functools.partial(<function _get_metadata at 0x7fdd20f7b430>, skipbadfiles=False, retries=0, xrootdtimeout=None, align_clusters=False)\n",
      "accumulator = set_accumulator()\n",
      "kwargs = {'compression': None, 'desc': 'Preprocessing', 'function_name': 'get_metadata', 'schema': <class 'coffea.nanoevents.schemas.nanoaod.NanoAODSchema'>, ...}\n",
      "dd = <module 'dask.dataframe' from '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/dask/dataframe/__init__.py'>\n",
      "\n",
      "    def dask_executor(items, function, accumulator, **kwargs):\n",
      "        \"\"\"Execute using dask futures\n",
      "    \n",
      "        Parameters\n",
      "        ----------\n",
      "            items : list\n",
      "                List of input arguments\n",
      "            function : callable\n",
      "                A function to be called on each input, which returns an accumulator instance\n",
      "            accumulator : Accumulatable\n",
      "                An accumulator to collect the output of the function\n",
      "            client : distributed.client.Client\n",
      "                A dask distributed client instance\n",
      "            treereduction : int, optional\n",
      "                Tree reduction factor for output accumulators (default: 20)\n",
      "            status : bool, optional\n",
      "                If true (default), enable progress bar\n",
      "            compression : int, optional\n",
      "                Compress accumulator outputs in flight with LZ4, at level specified (default 1)\n",
      "                Set to ``None`` for no compression.\n",
      "            priority : int, optional\n",
      "                Task priority, default 0\n",
      "            retries : int, optional\n",
      "                Number of retries for failed tasks (default: 3)\n",
      "            heavy_input : serializable, optional\n",
      "                Any value placed here will be broadcast to workers and joined to input\n",
      "                items in a tuple (item, heavy_input) that is passed to function.\n",
      "            function_name : str, optional\n",
      "                Name of the function being passed\n",
      "            use_dataframes: bool, optional\n",
      "                Retrieve output as a distributed Dask DataFrame (default: False).\n",
      "                The outputs of individual tasks must be Pandas DataFrames.\n",
      "    \n",
      "                .. note:: If ``heavy_input`` is set, ``function`` is assumed to be pure.\n",
      "        \"\"\"\n",
      "        import dask.dataframe as dd\n",
      "    \n",
      "        if len(items) == 0:\n",
      "            return accumulator\n",
      ">       client = kwargs.pop(\"client\")\n",
      "E       KeyError: 'client'\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:833: KeyError\n",
      "__________________ test_coffea_processor_adlexample1[200000] ___________________\n",
      "\n",
      "benchmark = <pytest_benchmark.fixture.BenchmarkFixture object at 0x7fdcb5c06e50>\n",
      "chunk_size = 200000\n",
      "\n",
      "    @pytest.mark.parametrize(\"chunk_size\", range(100000, 500000, 100000))\n",
      "    def test_coffea_processor_adlexample1(benchmark, chunk_size):\n",
      ">         benchmark(coffea_processor_adlexample1, chunk_size)\n",
      "\n",
      "<ipython-input-29-62d1ae0f702f>:3: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:127: in __call__\n",
      "    return self._raw(function_to_benchmark, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:149: in _raw\n",
      "    duration, iterations, loops_range = self._calibrate_timer(runner)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:277: in _calibrate_timer\n",
      "    duration = runner(loops_range)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:92: in runner\n",
      "    function_to_benchmark(*args, **kwargs)\n",
      "<ipython-input-27-255f1a1946c1>:2: in coffea_processor_adlexample1\n",
      "    output = processor.run_uproot_job(fileset,\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:1385: in run_uproot_job\n",
      "    out = pre_executor(to_get, metadata_fetcher, out, **pre_args)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "items = {<coffea.processor.executor.FileMeta object at 0x7fdcb5e1e140>}\n",
      "function = functools.partial(<function _get_metadata at 0x7fdd20f7b430>, skipbadfiles=False, retries=0, xrootdtimeout=None, align_clusters=False)\n",
      "accumulator = set_accumulator()\n",
      "kwargs = {'compression': None, 'desc': 'Preprocessing', 'function_name': 'get_metadata', 'schema': <class 'coffea.nanoevents.schemas.nanoaod.NanoAODSchema'>, ...}\n",
      "dd = <module 'dask.dataframe' from '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/dask/dataframe/__init__.py'>\n",
      "\n",
      "    def dask_executor(items, function, accumulator, **kwargs):\n",
      "        \"\"\"Execute using dask futures\n",
      "    \n",
      "        Parameters\n",
      "        ----------\n",
      "            items : list\n",
      "                List of input arguments\n",
      "            function : callable\n",
      "                A function to be called on each input, which returns an accumulator instance\n",
      "            accumulator : Accumulatable\n",
      "                An accumulator to collect the output of the function\n",
      "            client : distributed.client.Client\n",
      "                A dask distributed client instance\n",
      "            treereduction : int, optional\n",
      "                Tree reduction factor for output accumulators (default: 20)\n",
      "            status : bool, optional\n",
      "                If true (default), enable progress bar\n",
      "            compression : int, optional\n",
      "                Compress accumulator outputs in flight with LZ4, at level specified (default 1)\n",
      "                Set to ``None`` for no compression.\n",
      "            priority : int, optional\n",
      "                Task priority, default 0\n",
      "            retries : int, optional\n",
      "                Number of retries for failed tasks (default: 3)\n",
      "            heavy_input : serializable, optional\n",
      "                Any value placed here will be broadcast to workers and joined to input\n",
      "                items in a tuple (item, heavy_input) that is passed to function.\n",
      "            function_name : str, optional\n",
      "                Name of the function being passed\n",
      "            use_dataframes: bool, optional\n",
      "                Retrieve output as a distributed Dask DataFrame (default: False).\n",
      "                The outputs of individual tasks must be Pandas DataFrames.\n",
      "    \n",
      "                .. note:: If ``heavy_input`` is set, ``function`` is assumed to be pure.\n",
      "        \"\"\"\n",
      "        import dask.dataframe as dd\n",
      "    \n",
      "        if len(items) == 0:\n",
      "            return accumulator\n",
      ">       client = kwargs.pop(\"client\")\n",
      "E       KeyError: 'client'\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:833: KeyError\n",
      "__________________ test_coffea_processor_adlexample1[300000] ___________________\n",
      "\n",
      "benchmark = <pytest_benchmark.fixture.BenchmarkFixture object at 0x7fdcbdc0b0a0>\n",
      "chunk_size = 300000\n",
      "\n",
      "    @pytest.mark.parametrize(\"chunk_size\", range(100000, 500000, 100000))\n",
      "    def test_coffea_processor_adlexample1(benchmark, chunk_size):\n",
      ">         benchmark(coffea_processor_adlexample1, chunk_size)\n",
      "\n",
      "<ipython-input-29-62d1ae0f702f>:3: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:127: in __call__\n",
      "    return self._raw(function_to_benchmark, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:149: in _raw\n",
      "    duration, iterations, loops_range = self._calibrate_timer(runner)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:277: in _calibrate_timer\n",
      "    duration = runner(loops_range)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:92: in runner\n",
      "    function_to_benchmark(*args, **kwargs)\n",
      "<ipython-input-27-255f1a1946c1>:2: in coffea_processor_adlexample1\n",
      "    output = processor.run_uproot_job(fileset,\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:1385: in run_uproot_job\n",
      "    out = pre_executor(to_get, metadata_fetcher, out, **pre_args)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "items = {<coffea.processor.executor.FileMeta object at 0x7fdccea56180>}\n",
      "function = functools.partial(<function _get_metadata at 0x7fdd20f7b430>, skipbadfiles=False, retries=0, xrootdtimeout=None, align_clusters=False)\n",
      "accumulator = set_accumulator()\n",
      "kwargs = {'compression': None, 'desc': 'Preprocessing', 'function_name': 'get_metadata', 'schema': <class 'coffea.nanoevents.schemas.nanoaod.NanoAODSchema'>, ...}\n",
      "dd = <module 'dask.dataframe' from '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/dask/dataframe/__init__.py'>\n",
      "\n",
      "    def dask_executor(items, function, accumulator, **kwargs):\n",
      "        \"\"\"Execute using dask futures\n",
      "    \n",
      "        Parameters\n",
      "        ----------\n",
      "            items : list\n",
      "                List of input arguments\n",
      "            function : callable\n",
      "                A function to be called on each input, which returns an accumulator instance\n",
      "            accumulator : Accumulatable\n",
      "                An accumulator to collect the output of the function\n",
      "            client : distributed.client.Client\n",
      "                A dask distributed client instance\n",
      "            treereduction : int, optional\n",
      "                Tree reduction factor for output accumulators (default: 20)\n",
      "            status : bool, optional\n",
      "                If true (default), enable progress bar\n",
      "            compression : int, optional\n",
      "                Compress accumulator outputs in flight with LZ4, at level specified (default 1)\n",
      "                Set to ``None`` for no compression.\n",
      "            priority : int, optional\n",
      "                Task priority, default 0\n",
      "            retries : int, optional\n",
      "                Number of retries for failed tasks (default: 3)\n",
      "            heavy_input : serializable, optional\n",
      "                Any value placed here will be broadcast to workers and joined to input\n",
      "                items in a tuple (item, heavy_input) that is passed to function.\n",
      "            function_name : str, optional\n",
      "                Name of the function being passed\n",
      "            use_dataframes: bool, optional\n",
      "                Retrieve output as a distributed Dask DataFrame (default: False).\n",
      "                The outputs of individual tasks must be Pandas DataFrames.\n",
      "    \n",
      "                .. note:: If ``heavy_input`` is set, ``function`` is assumed to be pure.\n",
      "        \"\"\"\n",
      "        import dask.dataframe as dd\n",
      "    \n",
      "        if len(items) == 0:\n",
      "            return accumulator\n",
      ">       client = kwargs.pop(\"client\")\n",
      "E       KeyError: 'client'\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:833: KeyError\n",
      "__________________ test_coffea_processor_adlexample1[400000] ___________________\n",
      "\n",
      "benchmark = <pytest_benchmark.fixture.BenchmarkFixture object at 0x7fdcbdbfdf70>\n",
      "chunk_size = 400000\n",
      "\n",
      "    @pytest.mark.parametrize(\"chunk_size\", range(100000, 500000, 100000))\n",
      "    def test_coffea_processor_adlexample1(benchmark, chunk_size):\n",
      ">         benchmark(coffea_processor_adlexample1, chunk_size)\n",
      "\n",
      "<ipython-input-29-62d1ae0f702f>:3: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:127: in __call__\n",
      "    return self._raw(function_to_benchmark, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:149: in _raw\n",
      "    duration, iterations, loops_range = self._calibrate_timer(runner)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:277: in _calibrate_timer\n",
      "    duration = runner(loops_range)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pytest_benchmark/fixture.py:92: in runner\n",
      "    function_to_benchmark(*args, **kwargs)\n",
      "<ipython-input-27-255f1a1946c1>:2: in coffea_processor_adlexample1\n",
      "    output = processor.run_uproot_job(fileset,\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:1385: in run_uproot_job\n",
      "    out = pre_executor(to_get, metadata_fetcher, out, **pre_args)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "items = {<coffea.processor.executor.FileMeta object at 0x7fdca03b2580>}\n",
      "function = functools.partial(<function _get_metadata at 0x7fdd20f7b430>, skipbadfiles=False, retries=0, xrootdtimeout=None, align_clusters=False)\n",
      "accumulator = set_accumulator()\n",
      "kwargs = {'compression': None, 'desc': 'Preprocessing', 'function_name': 'get_metadata', 'schema': <class 'coffea.nanoevents.schemas.nanoaod.NanoAODSchema'>, ...}\n",
      "dd = <module 'dask.dataframe' from '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/dask/dataframe/__init__.py'>\n",
      "\n",
      "    def dask_executor(items, function, accumulator, **kwargs):\n",
      "        \"\"\"Execute using dask futures\n",
      "    \n",
      "        Parameters\n",
      "        ----------\n",
      "            items : list\n",
      "                List of input arguments\n",
      "            function : callable\n",
      "                A function to be called on each input, which returns an accumulator instance\n",
      "            accumulator : Accumulatable\n",
      "                An accumulator to collect the output of the function\n",
      "            client : distributed.client.Client\n",
      "                A dask distributed client instance\n",
      "            treereduction : int, optional\n",
      "                Tree reduction factor for output accumulators (default: 20)\n",
      "            status : bool, optional\n",
      "                If true (default), enable progress bar\n",
      "            compression : int, optional\n",
      "                Compress accumulator outputs in flight with LZ4, at level specified (default 1)\n",
      "                Set to ``None`` for no compression.\n",
      "            priority : int, optional\n",
      "                Task priority, default 0\n",
      "            retries : int, optional\n",
      "                Number of retries for failed tasks (default: 3)\n",
      "            heavy_input : serializable, optional\n",
      "                Any value placed here will be broadcast to workers and joined to input\n",
      "                items in a tuple (item, heavy_input) that is passed to function.\n",
      "            function_name : str, optional\n",
      "                Name of the function being passed\n",
      "            use_dataframes: bool, optional\n",
      "                Retrieve output as a distributed Dask DataFrame (default: False).\n",
      "                The outputs of individual tasks must be Pandas DataFrames.\n",
      "    \n",
      "                .. note:: If ``heavy_input`` is set, ``function`` is assumed to be pure.\n",
      "        \"\"\"\n",
      "        import dask.dataframe as dd\n",
      "    \n",
      "        if len(items) == 0:\n",
      "            return accumulator\n",
      ">       client = kwargs.pop(\"client\")\n",
      "E       KeyError: 'client'\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/coffea/processor/executor.py:833: KeyError\n",
      "------------------------- CSV report: coffea-bench.csv -------------------------\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_coffea_processor_adlexample1.py::test_coffea_processor_adlexample1[100000]\n",
      "FAILED test_coffea_processor_adlexample1.py::test_coffea_processor_adlexample1[200000]\n",
      "FAILED test_coffea_processor_adlexample1.py::test_coffea_processor_adlexample1[300000]\n",
      "FAILED test_coffea_processor_adlexample1.py::test_coffea_processor_adlexample1[400000]\n",
      "4 failed in 1.04s\n"
     ]
    }
   ],
   "source": [
    "ipytest.run(\"-qq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}