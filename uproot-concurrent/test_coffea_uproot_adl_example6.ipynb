{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "# (for .py version -> next line should be commented since they are converted to ipybn via jupytext)\n",
    "!pip install --user --upgrade coffea\n",
    "# Preparation for testing\n",
    "!pip install --user --upgrade ipytest\n",
    "!pip install --user --upgrade pytest-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.0.0/laurelin-1.0.0.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar\n",
    "!wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    import os\n",
    "    import ipytest\n",
    "    ipytest.config(rewrite_asserts=True, magics=True)\n",
    "    __file__ = 'test_coffea_uproot_adl_example6.ipynb'\n",
    "    # Run this cell before establishing spark connection <<<<< IMPORTANT\n",
    "    os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ':' + '/eos/user/o/oshadura/.local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'Trijets': { 'files': ['root://eospublic.cern.ch//eos/root-eos/benchmark/Run2012B_SingleMu.root'],\n",
    "             'treename': 'Events'\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrijetProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self._columns = ['MET_pt', 'nJet', 'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_mass', 'Jet_btag']\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        Jet_axis = hist.Bin(\"Jet_pt\", \"Jet [GeV]\", 50, 15, 200)\n",
    "        b_tag_axis = hist.Bin(\"b_tag\", \"b-tagging discriminant\", 50, 0, 1)\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'Jet_pt': hist.Hist(\"Counts\", dataset_axis, Jet_axis),\n",
    "            'b_tag': hist.Hist(\"Counts\", dataset_axis, b_tag_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int)\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = df[\"dataset\"]\n",
    "        \n",
    "        jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt=df['Jet_pt'].content,\n",
    "            eta=df['Jet_eta'].content,\n",
    "            phi=df['Jet_phi'].content,\n",
    "            mass=df['Jet_mass'].content,\n",
    "            b_tag=df['Jet_btag'].content\n",
    "            )\n",
    "        \n",
    "        # Closest calculates the distance from 172.5 of a group of masses, finds the minimum distance, then returns a Boolean array of the original input array shape with True where the minimum-distance mass is located.\n",
    "        def closest(masses):\n",
    "            delta = abs(172.5 - masses)\n",
    "            closest_masses = delta.min()\n",
    "            is_closest = (delta == closest_masses)\n",
    "            return is_closest\n",
    "        \n",
    "        # We're going to be generating combinations of three jets - that's a lot, and cutting pt off at 30 reduces jets by half.\n",
    "        cut_jets = jets[jets.pt > 30]\n",
    "        \n",
    "        # Get all combinations of three jets.\n",
    "        trijets = cut_jets.choose(3)\n",
    "        # Get combined masses of those combinations.\n",
    "        trijet_masses = trijets.mass\n",
    "        # Get the masses closest to specified value (see function above)\n",
    "        is_closest = closest(trijet_masses)\n",
    "        closest_trijets = trijets[is_closest]\n",
    "        # Get pt of the closest trijets.\n",
    "        closest_pt = closest_trijets.pt\n",
    "        # Get btag of the closest trijets. np.maximum(x,y) compares two arrays and gets element-wise maximums. We make two comparisons - once between the first and second jet, then between the first comparison and the third jet.\n",
    "        closest_btag = np.maximum(np.maximum(closest_trijets.i0['b_tag'], closest_trijets.i1['b_tag']), closest_trijets.i2['b_tag'])\n",
    "        \n",
    "        output['Jet_pt'].fill(dataset=dataset, Jet_pt=closest_pt.flatten())\n",
    "        output['b_tag'].fill(dataset=dataset, b_tag=closest_btag.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def coffea_uproot_adl_example6(n_workers, chunk_size, maxchunk_size):\n",
    "    output = processor.run_uproot_job(fileset,\n",
    "                                      treename = 'Events',\n",
    "                                      processor_instance = TrijetProcessor(),\n",
    "                                      executor = processor.futures_executor,\n",
    "                                      chunksize = chunk_size,\n",
    "                                      maxchunks = maxchunk_size,\n",
    "                                      executor_args = {'workers': n_workers}\n",
    "                                      \n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@pytest.mark.benchmark(group=\"coffea-uproot-adl-example6\")\n",
    "@pytest.mark.parametrize(\"n_workers\", range(1,psutil.cpu_count(logical=False)))\n",
    "@pytest.mark.parametrize(\"chunk_size\", range(200000,600000,200000))\n",
    "@pytest.mark.parametrize(\"maxchunk_size\", range(300000,700000,200000))\n",
    "def test_coffea_uproot_adl_example6(benchmark, n_workers, chunk_size, maxchunk_size):\n",
    "    benchmark(coffea_uproot_adl_example6, n_workers, chunk_size, maxchunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    ipytest.run('-qq')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
